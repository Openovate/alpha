#!/usr/bin/env python

import os
import sys
import argparse
import collections
import numpy as np
import random

sys.path.append('..')

from sklearn.feature_extraction import DictVectorizer
from sklearn import svm, preprocessing, cross_validation
from sklearn.metrics import precision_recall_curve, auc, classification_report, precision_recall_fscore_support

from alpha.lib import utils
from alpha.lib import processors
from alpha.lib import tokenizers
from alpha.lib import analyzers
from alpha.lib import clusterers

def main(args):
    # get the data path
    path = utils.get_data_path(args.site)
    # get the site urls
    urls = utils.load_urls(args.site)

    # load data
    pages = []

    for id, url in enumerate(urls):
        # site id
        site_id = url
        # get the site url
        url     = urls[url]['url']

        # valid url?
        if not url.strip():
            continue

        # load page data
        page = utils.load_raw_data(args.site, site_id)

        # initialize processor
        processor = processors.Processor([page], tokenizer=tokenizers.GenericTokenizer, analyzer=analyzers.LongestAnalyzer)
        # extract features
        features  = processor.extract()

        # cluster features
        clusterer = clusterers.DBSCAN()
        # get the labels
        labels = clusterer.cluster(features).labels_

        # collect clusters
        clusters = collections.defaultdict(list)
        # iterate on each texts and labels
        for text, label in zip(processor.texts, labels):
            # append text on clusters
            clusters[int(label)].append(text)

        # load the gold text from page
        gold_text = ' '.join(page['gold_text'])
        # tokenize gold text
        gold_text = processor.tokenizer.tokenize(gold_text)

        # max scoe
        max_score  = 0
        # total best score
        best_label = None

        # iterate on each clusters
        for label, texts in clusters.iteritems():
            # collect tokens
            tokens = ''

            # iterate on each text
            for text in texts:
                # append tokens
                tokens += text['tokens']

            # analyze and score token
            score = processor.analyzer.get_similarity(tokens, gold_text)

            # get the best score
            if score > max_score:
                # get new max score
                max_score  = score
                # get new best label
                best_label = label

        # all best lables, make it a positive sample
        for text in clusters[best_label]:
            # set as positive sample
            text['label'] = 1


        # collect page texts
        page_texts = []
        # iterate on each page texts
        for label, texts in clusters.iteritems():
            # combine page texts
            page_texts += texts

        # shuffle page text
        random.shuffle(page_texts)
        # append page text to pages
        pages.append(page_texts)

    # collect continuous features
    continuous_features = []
    # collect discrete features
    discrete_features   = []
    # collect labels
    labels = []

    # itreate on each pages
    for page in pages:
        # iterate on each text in page
        for text in page:
            # get the text length
            text_length  = len(text['tokens'])
            # calculate text area
            area         = text['bound']['height'] * text['bound']['width']

            try:
                # calculate text density
                text_density = float(text_length) / float(area)
            except:
                # set default density
                text_density = 0

            # continuous_feature
            continuous_feature = [text_length, text_density]
            # append continuos features
            continuous_features.append(continuous_feature)

            # discrete features
            discrete_feature         = dict()
            # get computed styles
            discrete_feature         = dict(text['computed'].items())
            # get tag path
            discrete_feature['path'] = ' > '.join(text['path'])
            # combine classess
            discrete_feature['class'] = ' > '.join([
                '%s%s' % (
                    selector['name'],
                    '.' + '.'.join(selector['classes']) if selector['classes'] else '',
                )
                for selector in text['selector']
            ])
            
            # append discrete feature
            discrete_features.append(discrete_feature)

            # label
            labels.append(text['label'])

    # vectorizer
    vectorizer          = DictVectorizer()
    # transform discrete features to vectorized data
    discrete_features   = vectorizer.fit_transform(discrete_features).toarray()
    # convert continuous features to array
    continuous_features = np.array(continuous_features)
    # convert labels to array
    labels              = np.array(labels).astype(np.float32)

    # scale features
    features = preprocessing.scale(features)

    # print features

    # stack up continuous and discrete features
    features = np.hstack([continuous_features, discrete_features]).astype(np.float32)
    
    precisions  = []
    recalls     = []
    f1scores    = []
    supports    = []

    # 4 fold cross validation
    rs = cross_validation.KFold(len(labels), n_folds=4, shuffle=False, random_state=0)
    # iterate on each cross validated
    for train_index, test_index in rs:
        print 'training size = %d, testing size = %d' % (len(train_index), len(test_index))

        # initialize svm classifier
        clf = svm.SVC(verbose=False, kernel='linear', probability=False, random_state=0, cache_size=2000, class_weight='balanced')
        # fit features, set labels
        clf.fit(features[train_index], labels[train_index])

        # print clf.n_support_
        
        # get predicted values for training
        # print "training:"
        predicted = clf.predict(features[train_index])
        # print classification_report(labels[train_index], predicted)

        # get predicted values for testing
        # print "testing:"
        predicted = clf.predict(features[test_index])
        # print classification_report(labels[test_index], predicted)

        # calculate precision, recall, f1score and support
        precision, recall, f1score, support = precision_recall_fscore_support(labels[test_index], predicted)

        # collect precisions
        precisions.append(precision)
        # collect recalls
        recalls.append(recall)
        # collect f1scores
        f1scores.append(f1score)
        # collect supports
        supports.append(support)

    # get mse
    precisions = np.mean(np.array(precisions), axis=0)
    # get mse
    recalls    = np.mean(np.array(recalls), axis=0)
    # get mse
    f1scores   = np.mean(np.array(f1scores), axis=0)
    # get mse
    supports   = np.mean(np.array(supports), axis=0)

    # output labels
    # for label in range(2):
        # print '%f\t%f\t%f\t%f' % (precisions[label], recalls[label], f1scores[label], supports[label])
    
    negatives = []
    positives = []
    for i in range(len(processor.texts)):
        # utils.pretty(processor.texts[i])
        if labels[i] == 1.0:
            positives.append(processor.texts[i]['text'])
        else:
            negatives.append(processor.texts[i]['text'])

    stats(negatives, positives)
    
    return

# show statistics
def stats(negatives, positives):
    # negative features
    negative_features  = set()
    # positive features
    positives_features = set()
    # negative total
    negative_counts  = collections.defaultdict(lambda: 0)
    # positive total
    positives_counts = collections.defaultdict(lambda: 0)

    # iterate on negatives
    for text in negatives:
        negative_features |= set(text['computed'].items())

    # iterate on positives
    for text in positives:
        positives_features |= set(text['computed'].items())

    common = negative_features & positives_features

    # iterate on negatives
    for text in negatives:
        for key, value in text['computed'].iteritems():
            if (key, value) not in common:
                negative_counts[(key, value)] += 1

    # iterate on positives
    for text in positives:
        for key, value in text['computed'].iteritems():
            if (key, value) not in common:
                positives_counts[(key, value)] += 1

    # print negatives
    print 'negatives: '
    utils.pretty(list(reversed(sorted(filter(lambda x: x[1] > 1, negative_counts.items()), key=lambda pair: pair[1])))[:10])
    # print positives
    print 'positives: '
    utils.pretty(list(reversed(sorted(filter(lambda x: x[1] > 1, positives_counts.items()), key=lambda pair: pair[1])))[:10])


# argument parser
def parse_args():
    # initialize argument parser
    parser = argparse.ArgumentParser(prog='extractor', description='SVM Classifier with KFold cross validation tool.')

    # set site argument
    parser.add_argument('-s', '--site', nargs='?', const=str, required=True, help='webpage data to classify')
   
    return parser.parse_args()

if __name__ == '__main__':
    main(parse_args())