#!/usr/bin/env python

import os
import sys
import argparse
import simplejson as json

# set root folder
sys.path.append('..')

from alpha.lib import utils
from alpha.lib import clusterers
from alpha.lib import processors

# confidence threshold
CONFIDENCE  = 0.5
# score threshold
SCORE       = 1000.0

def main(args):
    # get the site path
    path        = utils.get_data_path(args.site)
    # load urls file
    urls        = utils.load_urls(args.site)
    # get ids
    ids         = [id for i, id in enumerate(urls)]

    # keep track of clustered data
    clustered = []

    # if cluster folder does not exists
    if not os.path.exists(path['cluster']):
        # make that directory
        os.makedirs(path['cluster'])

    # if urls is not enough
    if len(urls) < 3:
        print 'not enough data for clustering, please extract atleast 3 different pages in ' + args.site
        sys.exit()

    # iterate on each urls
    for count in range(2, len(urls) + 1):
        # get the site id
        site_id = ids[count-1]

        print '[clusterer] clustering with %d to %s' % (count, site_id)

        # load data
        data = [utils.load_raw_data(args.site, ids[i]) for i in range(count)]
        data = data[:count-1]

        # process data
        processor = processors.Processor(data)
        features = processor.extract()

        # clustering
        clusterer = clusterers.DBSCAN()
        labels = clusterer.cluster(features).labels_

        # score
        clusters = processor.score(labels)

        # keep track of clustered data
        if len(clusters) > 0:
            clustered.append(clusters)

        # save clustered data
        with open(os.path.join(path['cluster'], '%s.json' % site_id), 'w') as f:
            f.write(json.dumps(clusters, indent=2, ensure_ascii=False).encode('utf8'))

        # selecting selectors
        selectors = []
        for cluster in clusters:
            if cluster['confidence'] < CONFIDENCE and cluster['score'] < SCORE:
                continue
            for key, selector in cluster['selectors'].iteritems():
                if selector and selector[-1]['name'] != 'a':
                    selectors.append({ 'path': key, 'tags' : selector })

        utils.pretty(selectors)

# argument parser
def parse_args():
    # initialize argument parser
    parser = argparse.ArgumentParser(prog='clusterer', description='Data DBSCAN clustering tool')

    # set site argument
    parser.add_argument('-s', '--site', nargs='?', const=str, required=True, help='extracted website data to cluster e.g galleon.ph')
   
    return parser.parse_args()

if __name__ == '__main__':
    main(parse_args())
